import json
import boto3
import psycopg2
import psycopg2.extras
import csv
import io
import os
import re
from urllib.parse import unquote_plus
from datetime import datetime

def parse_postgres_error(error, row_data, column_info):
    """PostgreSQLã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è§£æã—ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚’è¿”ã™"""
    error_info = {
        'error_type': 'UNKNOWN',
        'error_code': getattr(error, 'pgcode', 'UNKNOWN'),
        'error_message': str(error),
        'affected_columns': [],
        'details': []
    }
    
    error_msg = str(error)
    
    # NOT NULLåˆ¶ç´„é•å
    null_matches = re.findall(r'null value in column "([^"]+)" violates not-null constraint', error_msg)
    if null_matches:
        error_info['error_type'] = 'NOT_NULL_VIOLATION'
        for col in null_matches:
            error_info['affected_columns'].append(col)
            error_info['details'].append(f"ã‚«ãƒ©ãƒ  '{col}' ã«NULLå€¤ã¯è¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ä¸»ã‚­ãƒ¼ãƒ»ä¸€æ„åˆ¶ç´„é‡è¤‡
    pk_match = re.search(r'duplicate key value violates unique constraint "([^"]+)".*Key \(([^)]+)\)=\(([^)]+)\)', error_msg)
    if pk_match:
        constraint_name = pk_match.group(1)
        key_columns = pk_match.group(2)
        key_values = pk_match.group(3)
        
        # åˆ¶ç´„åã‹ã‚‰ç¨®é¡ã‚’åˆ¤å®š
        if 'pk' in constraint_name.lower() or 'pkey' in constraint_name.lower():
            error_info['error_type'] = 'PRIMARY_KEY_VIOLATION'
        else:
            error_info['error_type'] = 'UNIQUE_CONSTRAINT_VIOLATION'
        
        error_info['affected_columns'].append(key_columns)
        error_info['details'].append(f"åˆ¶ç´„ '{constraint_name}' é•å: ã‚­ãƒ¼({key_columns})=({key_values}) ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")
    
    # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•å
    fk_match = re.search(r'violates foreign key constraint "([^"]+)".*Key \(([^)]+)\)=\(([^)]+)\) is not present', error_msg)
    if fk_match:
        error_info['error_type'] = 'FOREIGN_KEY_VIOLATION'
        constraint_name = fk_match.group(1)
        key_columns = fk_match.group(2)
        key_values = fk_match.group(3)
        error_info['affected_columns'].append(key_columns)
        error_info['details'].append(f"å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ '{constraint_name}' é•å: å‚ç…§å…ˆã«å€¤({key_values})ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    # ãƒ‡ãƒ¼ã‚¿å‹ä¸ä¸€è‡´
    type_matches = re.findall(r'invalid input syntax for type (\w+): "([^"]+)"', error_msg)
    if type_matches:
        error_info['error_type'] = 'DATA_TYPE_MISMATCH'
        for data_type, value in type_matches:
            # ã©ã®ã‚«ãƒ©ãƒ ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‹æ¨æ¸¬
            for col, col_info in column_info.items():
                if col in row_data and str(row_data[col]) == value:
                    error_info['affected_columns'].append(col)
                    error_info['details'].append(f"ã‚«ãƒ©ãƒ  '{col}' (å‹: {data_type}): å€¤ '{value}' ã¯ç„¡åŠ¹ãªå½¢å¼ã§ã™")
                    break
    
    # æ–‡å­—åˆ—é•·è¶…é
    length_match = re.search(r'value too long for type character varying\((\d+)\)', error_msg)
    if length_match:
        error_info['error_type'] = 'STRING_LENGTH_EXCEEDED'
        max_length = length_match.group(1)
        # é•·ã™ãã‚‹å€¤ã‚’æŒã¤ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        for col, value in row_data.items():
            if value and len(str(value)) > int(max_length):
                error_info['affected_columns'].append(col)
                error_info['details'].append(f"ã‚«ãƒ©ãƒ  '{col}': å€¤ã®é•·ã• {len(str(value))} ãŒæœ€å¤§é•· {max_length} ã‚’è¶…é")
    
    # æ•°å€¤ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼
    overflow_match = re.search(r'numeric field overflow', error_msg)
    if overflow_match:
        error_info['error_type'] = 'NUMERIC_OVERFLOW'
        error_info['details'].append("æ•°å€¤ãŒè¨±å®¹ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™")
    
    # CHECKåˆ¶ç´„é•å
    check_match = re.search(r'new row for relation "([^"]+)" violates check constraint "([^"]+)"', error_msg)
    if check_match:
        error_info['error_type'] = 'CHECK_CONSTRAINT_VIOLATION'
        table_name = check_match.group(1)
        constraint_name = check_match.group(2)
        error_info['details'].append(f"CHECKåˆ¶ç´„ '{constraint_name}' é•å")
    
    return error_info

def format_error_details(row_number, row_data, error_info):
    """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ç®‡æ¡æ›¸ãå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = [
        f"\n{'='*60}",
        f"ã€ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€‘è¡Œç•ªå·: {row_number}",
        f"{'='*60}",
        f"â–  ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_info['error_type']}",
        f"â–  ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {error_info['error_code']}",
        f"â–  å½±éŸ¿ã‚’å—ã‘ãŸã‚«ãƒ©ãƒ : {', '.join(error_info['affected_columns']) if error_info['affected_columns'] else 'ä¸æ˜'}",
        f"â–  ã‚¨ãƒ©ãƒ¼è©³ç´°:"
    ]
    
    for detail in error_info['details']:
        lines.append(f"  - {detail}")
    
    lines.append(f"â–  å…ƒã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {error_info['error_message']}")
    lines.append(f"â–  è©²å½“è¡Œã®ãƒ‡ãƒ¼ã‚¿:")
    
    for col, value in row_data.items():
        if col in error_info['affected_columns']:
            lines.append(f"  - {col}: '{value}' â† â˜…å•é¡Œã®ã‚ã‚‹å€¤")
        else:
            lines.append(f"  - {col}: '{value}'")
    
    lines.append(f"{'='*60}\n")
    
    return '\n'.join(lines)

def send_sns_notification(sns_client, topic_arn, subject, message):
    """SNSé€šçŸ¥ã‚’é€ä¿¡ã™ã‚‹é–¢æ•°"""
    try:
        response = sns_client.publish(
            TopicArn=topic_arn,
            Subject=subject,
            Message=message
        )
        print(f"SNSé€šçŸ¥é€ä¿¡æˆåŠŸ: MessageId={response['MessageId']}")
        return True
    except Exception as e:
        print(f"SNSé€šçŸ¥é€ä¿¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_sns_message(file_name, table_name, total_rows, inserted_rows, failed_rows, error_summary):
    """SNSé€šçŸ¥ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ"""
    message = f"""
CSVå‡¦ç†çµæœé€šçŸ¥

ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}
ãƒ†ãƒ¼ãƒ–ãƒ«å: {table_name}
å‡¦ç†æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ã€å‡¦ç†çµæœã€‘
- ç·è¡Œæ•°: {total_rows}
- æˆåŠŸ: {inserted_rows}è¡Œ
- å¤±æ•—: {failed_rows}è¡Œ
- æˆåŠŸç‡: {(inserted_rows/total_rows*100):.1f}%

"""
    
    if error_summary:
        message += "ã€ã‚¨ãƒ©ãƒ¼å†…è¨³ã€‘\n"
        for error_type, info in error_summary.items():
            message += f"- {error_type}: {info['count']}ä»¶\n"
    
    return message

def lambda_handler(event, context):
    print("=== CSVå‡¦ç†Lambdaé–¢æ•°é–‹å§‹ ===")
    print(f"Event: {json.dumps(event, ensure_ascii=False)}")
    
    # ç’°å¢ƒå¤‰æ•°ã®ãƒ‡ãƒãƒƒã‚°
    print("=== ç’°å¢ƒå¤‰æ•° ===")
    print(f"DB_HOST: {os.environ.get('DB_HOST', 'NOT_SET')}")
    print(f"DB_PORT: {os.environ.get('DB_PORT', 'NOT_SET')}")
    print(f"DB_NAME: {os.environ.get('DB_NAME', 'NOT_SET')}")
    print(f"DB_USER: {os.environ.get('DB_USER', 'NOT_SET')}")
    print(f"DB_PASSWORD: {'SET' if os.environ.get('DB_PASSWORD') else 'NOT_SET'}")
    # print(f"SNS_TOPIC_ARN: {os.environ.get('SNS_TOPIC_ARN', 'NOT_SET')}")

    s3_client = boto3.client('s3')
    # sns_client = boto3.client('sns')

    db_host = os.environ['DB_HOST']
    db_port = os.environ['DB_PORT']
    db_name = os.environ['DB_NAME']
    db_user = os.environ['DB_USER']
    db_password = os.environ['DB_PASSWORD']
    # sns_topic_arn = os.environ.get('SNS_TOPIC_ARN')

    try:
        # S3ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰æƒ…å ±å–å¾—
        print("=== S3ã‚¤ãƒ™ãƒ³ãƒˆè§£æ ===")
        bucket_name = event['Records'][0]['s3']['bucket']['name']
        object_key = unquote_plus(event['Records'][0]['s3']['object']['key'])
        object_size = event['Records'][0]['s3']['object']['size']

        print(f"ãƒã‚±ãƒƒãƒˆå: {bucket_name}")
        print(f"ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼: {object_key}")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {object_size} bytes")
        print(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: s3://{bucket_name}/{object_key}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        print("=== S3ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«å–å¾— ===")
        s3_response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
        csv_content = s3_response['Body'].read().decode('utf-8')
        print(f"CSVå†…å®¹å–å¾—å®Œäº†: {len(csv_content)} æ–‡å­—")

        # CSVè§£æ
        print("=== CSVè§£æ ===")
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        rows = list(csv_reader)
        print(f"CSVè¡Œæ•°: {len(rows)}")
        
        if rows:
            print(f"CSVã‚«ãƒ©ãƒ : {list(rows[0].keys())}")
            print(f"æœ€åˆã®è¡Œãƒ‡ãƒ¼ã‚¿: {rows[0]}")

        if not rows:
            return {
                'statusCode': 200,
                'body': json.dumps({'message': 'CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'}, ensure_ascii=False)
            }

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ±ºå®š
        print("=== ãƒ†ãƒ¼ãƒ–ãƒ«åæ±ºå®š ===")
        file_name = object_key.split('/')[-1]
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}")
        
        file_name_without_ext = file_name.replace('.csv', '')
        print(f"æ‹¡å¼µå­é™¤å»å¾Œ: {file_name_without_ext}")
        
        # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã§åˆ†å‰²ã—ã¦ã€æ¥é ­è¾ä»¥é™ã‚’å–å¾—
        parts = file_name_without_ext.split('_', 1)
        print(f"åˆ†å‰²çµæœ: {parts}")
        
        if len(parts) >= 2:
            table_name = parts[1]  # æ¥é ­è¾ä»¥é™ãŒãƒ†ãƒ¼ãƒ–ãƒ«å
            print(f"æ¥é ­è¾é™¤å»å¾Œã®ãƒ†ãƒ¼ãƒ–ãƒ«å: {table_name}")
        else:
            # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ãŒãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åå…¨ä½“ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã™ã‚‹
            table_name = file_name_without_ext
            print(f"ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ãªã—ã€å…¨ä½“ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åã«: {table_name}")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã®æ­£è¦åŒ–ï¼ˆè‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰
        original_table_name = table_name
        table_name = ''.join(c for c in table_name if c.isalnum() or c == '_').lower()
        print(f"æ­£è¦åŒ–å‰: {original_table_name}")
        print(f"æ­£è¦åŒ–å¾Œï¼ˆå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰: {table_name}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        print("=== ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š ===")
        print(f"æ¥ç¶šå…ˆ: {db_host}:{db_port}/{db_name}")
        
        conn = psycopg2.connect(
            host=db_host,
            port=int(db_port),
            database=db_name,
            user=db_user,
            password=db_password,
            connect_timeout=30
        )
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ")

        conn.autocommit = False
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        print("=== ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª ===")
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = %s
            )
        """, (table_name,))
        
        table_exists = cursor.fetchone()['exists']
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' å­˜åœ¨: {table_exists}")
        
        if not table_exists:
            error_msg = f"ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚äº‹å‰ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            print(f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
            
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                ORDER BY table_name
            """)
            existing_tables = [row['table_name'] for row in cursor.fetchall()]
            print(f"æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§: {existing_tables}")
            
            cursor.close()
            conn.close()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®SNSé€šçŸ¥
            # if sns_topic_arn:
            #     error_subject = f"CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_name}"
            #     error_message = f"""
            # CSVå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚
            # 
            # ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}
            # ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(existing_tables)}
            # """
            #     send_sns_notification(sns_client, sns_topic_arn, error_subject, error_message)
            
            return {
                'statusCode': 400,
                'body': json.dumps({
                    'error': error_msg,
                    'table_name': table_name,
                    'existing_tables': existing_tables,
                    'source_file': f"s3://{bucket_name}/{object_key}"
                }, ensure_ascii=False)
            }

        # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ æƒ…å ±ã‚’å–å¾—ï¼ˆPRIMARY KEYã‚‚å«ã‚ã¦å–å¾—ï¼‰
        print("=== ãƒ†ãƒ¼ãƒ–ãƒ«ã‚«ãƒ©ãƒ æƒ…å ±å–å¾— ===")
        cursor.execute("""
            SELECT column_name, data_type, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = %s 
            AND column_name NOT IN ('file_source', 'processed_at')
            ORDER BY ordinal_position
        """, (table_name,))
        
        table_columns_info = cursor.fetchall()
        table_columns = [row['column_name'] for row in table_columns_info]
        
        print(f"æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ æ•°: {len(table_columns)}")
        print(f"æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ : {table_columns}")
        for col_info in table_columns_info:
            print(f"  - {col_info['column_name']}: {col_info['data_type']} (nullable: {col_info['is_nullable']}, default: {col_info['column_default']})")

        # CSVã®ã‚«ãƒ©ãƒ ã‚’å–å¾—
        csv_columns = list(rows[0].keys())
        print(f"CSVã®ã‚«ãƒ©ãƒ : {csv_columns}")

        # CSVã®ã‚«ãƒ©ãƒ ãŒå…¨ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        missing_columns = set(csv_columns) - set(table_columns)
        if missing_columns:
            print(f"è­¦å‘Š: CSVã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®ã‚«ãƒ©ãƒ ã¯ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}")
            print("å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿INSERTã—ã¾ã™")

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä½¿ç”¨
        insert_columns = [col for col in csv_columns if col in table_columns]
        print(f"INSERTå¯¾è±¡ã‚«ãƒ©ãƒ : {insert_columns}")
        
        if not insert_columns:
            error_msg = "CSVã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã§ä¸€è‡´ã™ã‚‹ã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“"
            print(f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
            cursor.close()
            conn.close()
            
            return {
                'statusCode': 400,
                'body': json.dumps({
                    'error': error_msg,
                    'csv_columns': csv_columns,
                    'table_columns': table_columns
                }, ensure_ascii=False)
            }

        # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥æº–å‚™
        print("=== ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥æº–å‚™ ===")
        
        # file_sourceã¨processed_atã‚’è¿½åŠ ï¼ˆã“ã‚Œã‚‰ã®ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = %s 
            AND column_name IN ('file_source', 'processed_at')
        """, (table_name,))
        
        system_columns = [row['column_name'] for row in cursor.fetchall()]
        print(f"ã‚·ã‚¹ãƒ†ãƒ ã‚«ãƒ©ãƒ : {system_columns}")
        
        # INSERTæ–‡ã®æ§‹ç¯‰
        all_columns = insert_columns.copy()
        placeholders = ['%s'] * len(insert_columns)
        
        if 'file_source' in system_columns:
            all_columns.append('file_source')
            placeholders.append('%s')
            print("file_sourceã‚«ãƒ©ãƒ ã‚’è¿½åŠ ")
        
        column_names = ', '.join([f'"{col}"' for col in all_columns])
        placeholders_str = ', '.join(placeholders)
        insert_sql = f'INSERT INTO "{table_name}" ({column_names}) VALUES ({placeholders_str})'
        
        print(f"INSERT SQL: {insert_sql}")

        # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
        print("=== ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥é–‹å§‹ ===")
        total_inserted = 0
        failed_rows = 0
        failed_details = []
        error_summary = {}  # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®é›†è¨ˆ
        
        # ã‚«ãƒ©ãƒ æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§ä¿æŒï¼ˆã‚¨ãƒ©ãƒ¼è§£æç”¨ï¼‰
        column_info = {col['column_name']: col for col in table_columns_info}
        
        for i, row in enumerate(rows):
            try:
                values = []
                for col in insert_columns:
                    value = row.get(col, '')
                    if value == '':
                        values.append(None)
                    else:
                        values.append(str(value))
                
                # file_sourceæƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
                if 'file_source' in system_columns:
                    values.append(f"s3://{bucket_name}/{object_key}")
                
                print(f"è¡Œ {i+1}: æŒ¿å…¥å€¤: {values}")
                
                cursor.execute(insert_sql, values)
                conn.commit()  # å„è¡Œã”ã¨ã«ã‚³ãƒŸãƒƒãƒˆ
                total_inserted += 1
                print(f"è¡Œ {i+1}: æŒ¿å…¥æˆåŠŸ")
                
                if (i + 1) % 100 == 0:
                    print(f"å‡¦ç†ä¸­: {i + 1}/{len(rows)} è¡Œ")
                    
            except psycopg2.Error as e:
                failed_rows += 1
                
                # PostgreSQLã‚¨ãƒ©ãƒ¼ã‚’è©³ç´°ã«è§£æ
                error_info = parse_postgres_error(e, row, column_info)
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
                error_log = format_error_details(i + 1, row, error_info)
                print(error_log)
                
                # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã«é›†è¨ˆ
                error_type = error_info['error_type']
                if error_type not in error_summary:
                    error_summary[error_type] = {
                        'count': 0,
                        'examples': []
                    }
                
                error_summary[error_type]['count'] += 1
                if len(error_summary[error_type]['examples']) < 3:
                    error_summary[error_type]['examples'].append({
                        'row_number': i + 1,
                        'affected_columns': error_info['affected_columns'],
                        'details': error_info['details']
                    })
                
                # æ—¢å­˜ã®failed_detailsã«ã‚‚è¿½åŠ 
                error_detail = {
                    'row_number': i + 1,
                    'error': str(e),
                    'error_info': error_info,
                    'values': values,
                    'row_data': row
                }
                failed_details.append(error_detail)
                
                conn.rollback()  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                
            except Exception as e:
                # PostgreSQLä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼
                failed_rows += 1
                error_detail = {
                    'row_number': i + 1,
                    'error': str(e),
                    'values': values,
                    'row_data': row
                }
                failed_details.append(error_detail)
                
                print(f"è¡Œ {i+1} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"å¤±æ•—ã—ãŸå€¤: {values}")
                print(f"å…ƒã®ãƒ‡ãƒ¼ã‚¿: {row}")
                
                conn.rollback()
        
        # ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
        if error_summary:
            print("\n" + "="*60)
            print("ã€ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã€‘")
            print("="*60)
            for error_type, info in error_summary.items():
                print(f"\nâ–  {error_type}: {info['count']}ä»¶")
                for idx, example in enumerate(info['examples'], 1):
                    print(f"  ä¾‹{idx}) è¡Œ {example['row_number']}:")
                    print(f"     å½±éŸ¿ã‚«ãƒ©ãƒ : {', '.join(example['affected_columns'])}")
                    for detail in example['details']:
                        print(f"     - {detail}")
            print("="*60 + "\n")
        
        print(f"=== ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥å®Œäº† ===")
        print(f"æˆåŠŸ: {total_inserted}è¡Œ")
        print(f"å¤±æ•—: {failed_rows}è¡Œ")
        
        # å‡¦ç†çµæœç¢ºèª
        print("=== æœ€çµ‚çµæœç¢ºèª ===")
        cursor.execute(f'SELECT COUNT(*) as count FROM "{table_name}"')
        result = cursor.fetchone()
        final_count = result['count'] if result else 0
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' ã®æœ€çµ‚è¡Œæ•°: {final_count}")
        
        cursor.close()
        conn.close()

        success_message = f"CSVå‡¦ç†å®Œäº†: {total_inserted}è¡ŒæŒ¿å…¥, {failed_rows}è¡Œå¤±æ•—, ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' æœ€çµ‚è¡Œæ•°: {final_count}"
        print(f"=== å‡¦ç†çµæœ ===")
        print(success_message)

        # SNSé€šçŸ¥ã®é€ä¿¡
        # if sns_topic_arn:
        #     # æˆåŠŸç‡ã«å¿œã˜ã¦é€šçŸ¥ãƒ¬ãƒ™ãƒ«ã‚’å¤‰æ›´
        #     success_rate = (total_inserted / len(rows) * 100) if len(rows) > 0 else 0
        #     
        #     if success_rate == 100:
        #         subject = f"âœ… CSVå‡¦ç†æˆåŠŸ: {file_name}"
        #     elif success_rate >= 80:
        #         subject = f"âš ï¸ CSVå‡¦ç†ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼: {file_name} ({success_rate:.1f}%æˆåŠŸ)"
        #     else:
        #         subject = f"âŒ CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼å¤šæ•°: {file_name} ({success_rate:.1f}%æˆåŠŸ)"
        #     
        #     message = create_sns_message(
        #         file_name, table_name, len(rows), 
        #         total_inserted, failed_rows, error_summary
        #     )
        #     
        #     send_sns_notification(sns_client, sns_topic_arn, subject, message)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ã« error_summary ã‚’è¿½åŠ 
        response_body = {
            'message': success_message,
            'table_name': table_name,
            'source_file': f"s3://{bucket_name}/{object_key}",
            'total_rows': len(rows),
            'inserted_rows': total_inserted,
            'failed_rows': failed_rows,
            'final_table_count': final_count,
            'matched_columns': insert_columns,
            'missing_columns': list(missing_columns) if missing_columns else [],
            'failed_details': failed_details[:10]  # æœ€åˆã®10ä»¶ã®ã‚¨ãƒ©ãƒ¼è©³ç´°
        }
        
        # error_summaryãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if error_summary:
            response_body['error_summary'] = error_summary

        return {
            'statusCode': 200,
            'body': json.dumps(response_body, ensure_ascii=False)
        }

    except Exception as e:
        print(f"=== äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ ===")
        print(f"CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        
        # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼æ™‚ã®SNSé€šçŸ¥
        # if 'sns_topic_arn' in locals() and sns_topic_arn:
        #     critical_subject = f"ğŸš¨ CSVå‡¦ç†ã§é‡å¤§ã‚¨ãƒ©ãƒ¼: {file_name if 'file_name' in locals() else 'unknown'}"
        #     critical_message = f"""
        # CSVå‡¦ç†ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚
        # 
        # ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}
        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}
        # 
        # è©³ç´°ã¯CloudWatch Logsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
        # """
        #     send_sns_notification(sns_client, sns_topic_arn, critical_subject, critical_message)
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': f'CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}',
                'error_type': type(e).__name__,
                'error_detail': traceback.format_exc(),
                'source_file': f"s3://{bucket_name}/{object_key}" if 'bucket_name' in locals() else 'unknown'
            }, ensure_ascii=False)
        }