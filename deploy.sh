#!/bin/bash

# ETL CSV to RDS PostgreSQL System ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®Œå…¨ç‰ˆãƒ»æ”¹å–„ç‰ˆï¼‰
# UTF-8ã§ä¿å­˜ã—ã¦ãã ã•ã„

set -e

# è¨­å®šå¤‰æ•°
STACK_NAME="etl-csv-to-rds-postgresql"
TEMPLATE_FILE="etl-csv-to-rds-postgresql.yaml"
REGION="us-east-2"
PROJECT_NAME="etl-csv-to-rds-postgresql"
DB_PASSWORD="TestPassword123!"
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# è‰²ä»˜ãå‡ºåŠ›ç”¨
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[0;33m'
RED='\033[0;31m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_step() {
    echo -e "${PURPLE}[STEP]${NC} $1"
}

print_debug() {
    echo -e "${CYAN}[DEBUG]${NC} $1"
}

# å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯é–¢æ•°
check_prerequisites() {
    print_step "=== å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ ==="
    
    # AWS CLIç¢ºèª
    if ! command -v aws &> /dev/null; then
        print_error "AWS CLIãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        echo "AWS CLIã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: https://aws.amazon.com/cli/"
        exit 1
    fi
    
    # AWS CLIãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
    AWS_CLI_VERSION=$(aws --version 2>&1 | head -n 1)
    print_success "AWS CLI: $AWS_CLI_VERSION"
    
    # AWS CLI v1ã®å ´åˆã®è­¦å‘Š
    if echo "$AWS_CLI_VERSION" | grep -q "aws-cli/1\."; then
        print_warning "AWS CLI v1ã‚’ä½¿ç”¨ä¸­ã€‚v2ã¸ã®æ›´æ–°ã‚’æŽ¨å¥¨ã—ã¾ã™ã€‚"
        print_info "v2ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"
    fi
    
    # AWSèªè¨¼ç¢ºèª
    if ! aws sts get-caller-identity &> /dev/null; then
        print_error "AWSèªè¨¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
        echo "aws configure ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„"
        exit 1
    fi
    
    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±è¡¨ç¤º
    CURRENT_USER=$(aws sts get-caller-identity --query 'Arn' --output text)
    print_success "AWSèªè¨¼: ${CURRENT_USER}"
    
    # CloudFormationæ“ä½œæ¨©é™ç¢ºèª
    print_info "CloudFormationæ¨©é™ãƒã‚§ãƒƒã‚¯ä¸­..."
    if aws cloudformation list-stacks --region "${REGION}" --max-items 1 > /dev/null 2>&1; then
        print_success "CloudFormationæ¨©é™: OK"
    else
        print_error "CloudFormationã®æ“ä½œæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“"
        exit 1
    fi
    
    # ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã®åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³ç¢ºèª
    AZ_COUNT=$(aws ec2 describe-availability-zones --region ${REGION} --query 'length(AvailabilityZones)' --output text)
    if [ "$AZ_COUNT" -lt 3 ]; then
        print_warning "ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ ${REGION} ã®åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³ãŒ3ã¤æœªæº€ã§ã™ (ç¾åœ¨: ${AZ_COUNT})"
        print_info "RDSç”¨ã«æœ€ä½Ž2ã¤ã€æŽ¨å¥¨3ã¤ã®åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³ãŒå¿…è¦ã§ã™"
        
        # åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³ä¸€è¦§è¡¨ç¤º
        print_info "åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³ä¸€è¦§:"
        aws ec2 describe-availability-zones --region ${REGION} --query 'AvailabilityZones[].{Name:ZoneName,State:State}' --output table
        
        if [ "$AZ_COUNT" -lt 2 ]; then
            print_error "åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³ãŒ2ã¤æœªæº€ã®ãŸã‚ã€RDSãŒä½œæˆã§ãã¾ã›ã‚“"
            exit 1
        fi
    else
        print_success "åˆ©ç”¨å¯èƒ½ã‚¾ãƒ¼ãƒ³: ${AZ_COUNT}å€‹ï¼ˆååˆ†ï¼‰"
    fi
    
    # Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
    if command -v python3 &> /dev/null; then
        PYTHON_VERSION=$(python3 --version 2>&1)
        print_success "Python: ${PYTHON_VERSION}"
    else
        print_warning "Python3ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆJSONæ•´å½¢ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰"
    fi
    
    # zipã‚³ãƒžãƒ³ãƒ‰ç¢ºèª
    if ! command -v zip &> /dev/null; then
        print_error "zipã‚³ãƒžãƒ³ãƒ‰ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        exit 1
    fi
    print_success "zip: ç¢ºèªæ¸ˆã¿"
    
    # psqlã‚³ãƒžãƒ³ãƒ‰ç¢ºèªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if command -v psql &> /dev/null; then
        PSQL_VERSION=$(psql --version | head -n 1)
        print_success "PostgreSQL Client: ${PSQL_VERSION}"
    else
        print_warning "psqlã‚³ãƒžãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›´æŽ¥æŽ¥ç¶šã«å½±éŸ¿ã—ã¾ã™ï¼‰"
        print_info "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: sudo apt-get install postgresql-client (Ubuntu/Debian)"
        print_info "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: brew install postgresql (macOS)"
    fi
    
    print_success "å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†"
}

# VPCè¨­å®šæ¤œè¨¼é–¢æ•°
validate_vpc_configuration() {
    print_step "=== VPCè¨­å®šæ¤œè¨¼ ==="
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if [ ! -f "${TEMPLATE_FILE}" ]; then
        print_error "CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ${TEMPLATE_FILE}"
        exit 1
    fi
    print_success "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: ${TEMPLATE_FILE} ç¢ºèªæ¸ˆã¿"
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    print_info "CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ä¸­..."
    if aws cloudformation validate-template \
        --template-body "file://${TEMPLATE_FILE}" \
        --region "${REGION}" > /dev/null 2>&1; then
        print_success "CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡: OK"
    else
        print_error "CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼"
        aws cloudformation validate-template \
            --template-body "file://${TEMPLATE_FILE}" \
            --region "${REGION}" 2>&1
        exit 1
    fi
    
    # VPCã‚¯ã‚©ãƒ¼ã‚¿ç¢ºèª
    print_info "VPCã‚¯ã‚©ãƒ¼ã‚¿ç¢ºèªä¸­..."
    VPC_COUNT=$(aws ec2 describe-vpcs --region ${REGION} --query 'length(Vpcs)' --output text)
    VPC_LIMIT=$(aws service-quotas get-service-quota --service-code ec2 --quota-code L-F678F1CE --region ${REGION} --query 'Quota.Value' --output text 2>/dev/null || echo "5")
    
    print_debug "ç¾åœ¨ã®VPCæ•°: ${VPC_COUNT}/${VPC_LIMIT}"
    if [ "$VPC_COUNT" -ge "$VPC_LIMIT" ]; then
        print_warning "VPCåˆ¶é™ã«è¿‘ã¥ã„ã¦ã„ã¾ã™ (${VPC_COUNT}/${VPC_LIMIT})"
    fi
    
    print_success "VPCè¨­å®šæ¤œè¨¼å®Œäº†"
}

# å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
check_required_files() {
    print_step "=== å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª ==="
    local missing_files=()
    
    # å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«
    if [ ! -f "psycopg2-layer-python311-fixed.zip" ]; then
        missing_files+=("psycopg2-layer-python311-fixed.zip")
    fi
    
    if [ ! -f "${TEMPLATE_FILE}" ]; then
        missing_files+=("${TEMPLATE_FILE}")
    fi
    
    # Pythonãƒ•ã‚¡ã‚¤ãƒ«
    local python_files=("table_creator.py" "csv_processor.py" "query_executor.py")
    for py_file in "${python_files[@]}"; do
        if [ ! -f "$py_file" ]; then
            missing_files+=("$py_file")
        else
            # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
            if python3 -m py_compile "$py_file" 2>/dev/null; then
                print_success "Pythonæ§‹æ–‡ãƒã‚§ãƒƒã‚¯: $py_file OK"
            else
                print_warning "Pythonæ§‹æ–‡è­¦å‘Š: $py_file ï¼ˆã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ï¼‰"
            fi
        fi
    done
    
    if [ ${#missing_files[@]} -gt 0 ]; then
        print_error "å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:"
        for file in "${missing_files[@]}"; do
            echo "  - $file"
        done
        exit 1
    fi
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
    print_info "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª:"
    for file in "psycopg2-layer-python311-fixed.zip" "${python_files[@]}"; do
        if [ -f "$file" ]; then
            SIZE=$(du -h "$file" | cut -f1)
            print_debug "  $file: ${SIZE}"
        fi
    done
    
    # SQLãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    local sql_count=0
    print_info "SQLãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºçµæžœ:"
    for file in *.sql; do
        if [ -f "$file" ]; then
            SIZE=$(du -h "$file" | cut -f1)
            echo "  âœ“ $file (${SIZE})"
            ((sql_count++))
            
            # SQLãƒ•ã‚¡ã‚¤ãƒ«ã®ç°¡å˜ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
            if grep -qi "CREATE TABLE" "$file"; then
                print_debug "    CREATE TABLEæ–‡ã‚’æ¤œå‡º"
            fi
            if grep -qi "INSERT INTO" "$file"; then
                print_debug "    INSERTæ–‡ã‚’æ¤œå‡º"
            fi
        fi
    done
    
    if [ $sql_count -eq 0 ]; then
        print_warning "SQLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚"
        print_info "å¾Œã§SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambdaé–¢æ•°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        return 1
    else
        print_success "SQLãƒ•ã‚¡ã‚¤ãƒ«: ${sql_count}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ"
        return 0
    fi
}

# Lambdaé–¢æ•°ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
create_lambda_zips() {
    print_step "=== Lambdaé–¢æ•°ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ==="
    
    # æ—¢å­˜ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    rm -f table_creator.zip csv_processor.zip query_executor.zip 2>/dev/null || true
    
    # table_creator.zip
    print_info "table_creator.zipä½œæˆä¸­..."
    if zip -j table_creator.zip table_creator.py > /dev/null 2>&1; then
        ZIP_SIZE=$(du -h table_creator.zip | cut -f1)
        print_success "table_creator.zipä½œæˆå®Œäº† (${ZIP_SIZE})"
    else
        print_error "table_creator.zipä½œæˆå¤±æ•—"
        exit 1
    fi
    
    # csv_processor.zip
    print_info "csv_processor.zipä½œæˆä¸­..."
    if zip -j csv_processor.zip csv_processor.py > /dev/null 2>&1; then
        ZIP_SIZE=$(du -h csv_processor.zip | cut -f1)
        print_success "csv_processor.zipä½œæˆå®Œäº† (${ZIP_SIZE})"
    else
        print_error "csv_processor.zipä½œæˆå¤±æ•—"
        exit 1
    fi
    
    # query_executor.zip
    print_info "query_executor.zipä½œæˆä¸­..."
    if zip -j query_executor.zip query_executor.py > /dev/null 2>&1; then
        ZIP_SIZE=$(du -h query_executor.zip | cut -f1)
        print_success "query_executor.zipä½œæˆå®Œäº† (${ZIP_SIZE})"
    else
        print_error "query_executor.zipä½œæˆå¤±æ•—"
        exit 1
    fi
    
    print_success "Lambdaé–¢æ•°ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†"
}

# Change Setä½œæˆãƒ»å®Ÿè¡Œé–¢æ•°
deploy_with_changeset() {
    local changeset_name="update-$(date +%Y%m%d-%H%M%S)"
    
    print_step "=== Change Setä½¿ç”¨ã«ã‚ˆã‚‹ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–° ==="
    print_info "Change Setä½œæˆä¸­: ${changeset_name}"
    
    # Change Setä½œæˆ
    if aws cloudformation create-change-set \
        --stack-name "${STACK_NAME}" \
        --template-body "file://${TEMPLATE_FILE}" \
        --change-set-name "${changeset_name}" \
        --parameters \
            ParameterKey=ProjectName,ParameterValue="${PROJECT_NAME}" \
            ParameterKey=DBMasterPassword,ParameterValue="${DB_PASSWORD}" \
        --capabilities CAPABILITY_IAM \
        --region "${REGION}" > /dev/null 2>&1; then
        print_success "Change Setä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡"
    else
        print_error "Change Setä½œæˆå¤±æ•—"
        return 1
    fi
    
    # Change Setä½œæˆå®Œäº†å¾…æ©Ÿ
    print_info "Change Setä½œæˆå®Œäº†å¾…æ©Ÿä¸­..."
    local wait_count=0
    while [ $wait_count -lt 30 ]; do
        STATUS=$(aws cloudformation describe-change-set \
            --stack-name "${STACK_NAME}" \
            --change-set-name "${changeset_name}" \
            --region "${REGION}" \
            --query 'Status' \
            --output text 2>/dev/null || echo "PENDING")
        
        if [ "$STATUS" = "CREATE_COMPLETE" ]; then
            print_success "Change Setä½œæˆå®Œäº†"
            break
        elif [ "$STATUS" = "FAILED" ]; then
            print_error "Change Setä½œæˆå¤±æ•—"
            REASON=$(aws cloudformation describe-change-set \
                --stack-name "${STACK_NAME}" \
                --change-set-name "${changeset_name}" \
                --region "${REGION}" \
                --query 'StatusReason' \
                --output text 2>/dev/null || echo "ä¸æ˜Ž")
            print_error "å¤±æ•—ç†ç”±: ${REASON}"
            return 1
        fi
        
        echo -n "."
        sleep 5
        ((wait_count++))
    done
    
    if [ $wait_count -ge 30 ]; then
        print_error "Change Setä½œæˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
        return 1
    fi
    
    # Change Setå†…å®¹è¡¨ç¤º
    print_info "Change Setå†…å®¹:"
    echo ""
    aws cloudformation describe-change-set \
        --stack-name "${STACK_NAME}" \
        --change-set-name "${changeset_name}" \
        --region "${REGION}" \
        --query 'Changes[].{Action:Action,ResourceType:ResourceChange.ResourceType,LogicalId:ResourceChange.LogicalResourceId,Replacement:ResourceChange.Replacement}' \
        --output table 2>/dev/null || echo "å¤‰æ›´å†…å®¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
    
    # å¤‰æ›´æ•°ç¢ºèª
    CHANGE_COUNT=$(aws cloudformation describe-change-set \
        --stack-name "${STACK_NAME}" \
        --change-set-name "${changeset_name}" \
        --region "${REGION}" \
        --query 'length(Changes)' \
        --output text 2>/dev/null || echo "0")
    
    print_info "å¤‰æ›´é …ç›®æ•°: ${CHANGE_COUNT}"
    
    if [ "$CHANGE_COUNT" = "0" ]; then
        print_info "å¤‰æ›´ãŒãªã„ãŸã‚ã€Change Setã‚’å‰Šé™¤ã—ã¾ã™"
        aws cloudformation delete-change-set \
            --stack-name "${STACK_NAME}" \
            --change-set-name "${changeset_name}" \
            --region "${REGION}" > /dev/null 2>&1
        return 2  # å¤‰æ›´ãªã—ã‚’ç¤ºã™ç‰¹åˆ¥ãªãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰
    fi
    
    # å®Ÿè¡Œç¢ºèª
    echo ""
    echo -e "${YELLOW}Change Setã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ${NC}"
    echo "  - å¤‰æ›´é …ç›®æ•°: ${CHANGE_COUNT}"
    echo "  - ã‚¹ã‚¿ãƒƒã‚¯å: ${STACK_NAME}"
    echo "  - ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: ${REGION}"
    echo ""
    read -p "å®Ÿè¡Œã™ã‚‹å ´åˆã¯ 'yes' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: " confirm
    
    if [[ $confirm = "yes" ]]; then
        print_info "Change Setå®Ÿè¡Œä¸­..."
        if aws cloudformation execute-change-set \
            --stack-name "${STACK_NAME}" \
            --change-set-name "${changeset_name}" \
            --region "${REGION}" > /dev/null 2>&1; then
            print_success "Change Setå®Ÿè¡Œé–‹å§‹"
        else
            print_error "Change Setå®Ÿè¡Œå¤±æ•—"
            return 1
        fi
        
        print_info "ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–°å®Œäº†å¾…æ©Ÿä¸­..."
        print_warning "ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™..."
        
        if aws cloudformation wait stack-update-complete \
            --stack-name "${STACK_NAME}" \
            --region "${REGION}"; then
            print_success "Change Setå®Ÿè¡Œå®Œäº†"
            return 0
        else
            print_error "ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ"
            print_info "CloudFormationã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            return 1
        fi
    else
        print_info "Change Setå‰Šé™¤ä¸­..."
        aws cloudformation delete-change-set \
            --stack-name "${STACK_NAME}" \
            --change-set-name "${changeset_name}" \
            --region "${REGION}" > /dev/null 2>&1
        print_info "Change Setã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ"
        return 1
    fi
}

# ã‚¹ã‚¿ãƒƒã‚¯çŠ¶æ…‹ç¢ºèª
check_stack_status() {
    print_step "=== ã‚¹ã‚¿ãƒƒã‚¯çŠ¶æ…‹ç¢ºèª ==="
    
    STACK_STATUS=$(aws cloudformation describe-stacks \
        --stack-name "${STACK_NAME}" \
        --region "${REGION}" \
        --query 'Stacks[0].StackStatus' \
        --output text 2>/dev/null || echo "NOT_EXISTS")
    
    case "$STACK_STATUS" in
        "NOT_EXISTS")
            print_info "æ–°è¦ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆãƒ¢ãƒ¼ãƒ‰"
            return 0
            ;;
        "CREATE_COMPLETE"|"UPDATE_COMPLETE")
            print_info "æ—¢å­˜ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ (çŠ¶æ…‹: ${STACK_STATUS})"
            return 1
            ;;
        "CREATE_IN_PROGRESS"|"UPDATE_IN_PROGRESS")
            print_warning "ã‚¹ã‚¿ãƒƒã‚¯å‡¦ç†ä¸­ (çŠ¶æ…‹: ${STACK_STATUS})"
            print_info "å‡¦ç†å®Œäº†ã¾ã§å¾…æ©Ÿã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„"
            exit 1
            ;;
        "CREATE_FAILED"|"UPDATE_FAILED"|"ROLLBACK_COMPLETE"|"UPDATE_ROLLBACK_COMPLETE"|"ROLLBACK_FAILED")
            print_warning "ã‚¹ã‚¿ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ (çŠ¶æ…‹: ${STACK_STATUS})"
            print_info "å¤±æ•—ã—ãŸã‚¹ã‚¿ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æŽ¨å¥¨ã—ã¾ã™"
            echo ""
            print_info "å¤±æ•—ã®è©³ç´°:"
            aws cloudformation describe-stack-events \
                --stack-name "${STACK_NAME}" \
                --region "${REGION}" \
                --max-items 5 \
                --query 'StackEvents[?ResourceStatus==`CREATE_FAILED` || ResourceStatus==`UPDATE_FAILED`].{Time:Timestamp,Resource:LogicalResourceId,Reason:ResourceStatusReason}' \
                --output table 2>/dev/null || echo "è©³ç´°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
            
            echo ""
            read -p "å¤±æ•—ã—ãŸã‚¹ã‚¿ãƒƒã‚¯ã‚’è‡ªå‹•å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): " delete_confirm
            if [[ $delete_confirm =~ ^[Yy]$ ]]; then
                print_info "ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤ä¸­..."
                if aws cloudformation delete-stack \
                    --stack-name "${STACK_NAME}" \
                    --region "${REGION}" > /dev/null 2>&1; then
                    
                    print_info "ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤å®Œäº†å¾…æ©Ÿä¸­..."
                    local delete_wait_count=0
                    while [ $delete_wait_count -lt 20 ]; do  # æœ€å¤§10åˆ†å¾…æ©Ÿ
                        DELETE_STATUS=$(aws cloudformation describe-stacks \
                            --stack-name "${STACK_NAME}" \
                            --region "${REGION}" \
                            --query 'Stacks[0].StackStatus' \
                            --output text 2>/dev/null || echo "DELETED")
                        
                        if [ "$DELETE_STATUS" = "DELETED" ]; then
                            print_success "ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤å®Œäº†"
                            return 0  # æ–°è¦ä½œæˆãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
                        elif [ "$DELETE_STATUS" = "DELETE_FAILED" ]; then
                            print_error "ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤å¤±æ•—"
                            exit 1
                        fi
                        
                        echo -n "."
                        sleep 30
                        ((delete_wait_count++))
                    done
                    
                    print_error "ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
                    exit 1
                else
                    print_error "ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤ã‚³ãƒžãƒ³ãƒ‰ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ"
                    exit 1
                fi
            else
                print_info "æ‰‹å‹•ã§ã‚¹ã‚¿ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„:"
                echo "aws cloudformation delete-stack --stack-name ${STACK_NAME} --region ${REGION}"
                exit 1
            fi
            ;;
        *)
            print_warning "ä¸æ˜Žãªã‚¹ã‚¿ãƒƒã‚¯çŠ¶æ…‹: ${STACK_STATUS}"
            return 1
            ;;
    esac
}

# ä¸€æ™‚S3ãƒã‚±ãƒƒãƒˆç®¡ç†
manage_temp_bucket() {
    print_step "=== ä¸€æ™‚S3ãƒã‚±ãƒƒãƒˆç®¡ç† ==="
    
    TEMP_BUCKET_NAME="etl-csv-to-rds-postgresql-temp-files-${ACCOUNT_ID}"
    
    # ä¸€æ™‚ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if aws s3 ls "s3://${TEMP_BUCKET_NAME}" > /dev/null 2>&1; then
        print_info "ä¸€æ™‚S3ãƒã‚±ãƒƒãƒˆæ—¢å­˜: ${TEMP_BUCKET_NAME}"
        
        # ãƒã‚±ãƒƒãƒˆå†…å®¹ç¢ºèª
        OBJECT_COUNT=$(aws s3 ls "s3://${TEMP_BUCKET_NAME}" --recursive --region ${REGION} | wc -l)
        print_debug "ãƒã‚±ãƒƒãƒˆå†…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: ${OBJECT_COUNT}"
    else
        print_info "ä¸€æ™‚S3ãƒã‚±ãƒƒãƒˆä½œæˆä¸­: ${TEMP_BUCKET_NAME}"
        if aws s3 mb "s3://${TEMP_BUCKET_NAME}" --region ${REGION} > /dev/null 2>&1; then
            print_success "ä¸€æ™‚S3ãƒã‚±ãƒƒãƒˆä½œæˆå®Œäº†"
        else
            print_error "ä¸€æ™‚S3ãƒã‚±ãƒƒãƒˆä½œæˆå¤±æ•—"
            exit 1
        fi
    fi
}

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
upload_files() {
    print_step "=== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ==="
    
    local upload_errors=0
    
    # Lambdaé–¢æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    local lambda_files=("table_creator.zip" "csv_processor.zip" "query_executor.zip")
    for zip_file in "${lambda_files[@]}"; do
        print_info "${zip_file}ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."
        if aws s3 cp "$zip_file" "s3://${TEMP_BUCKET_NAME}/lambda-code/$zip_file" --region ${REGION} > /dev/null 2>&1; then
            print_success "${zip_file}ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†"
        else
            print_error "${zip_file}ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—"
            ((upload_errors++))
        fi
    done
    
    # psycopg2ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    print_info "psycopg2ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."
    if aws s3 cp psycopg2-layer-python311-fixed.zip "s3://${TEMP_BUCKET_NAME}/layers/psycopg2-layer-python311-fixed.zip" --region ${REGION} > /dev/null 2>&1; then
        print_success "psycopg2ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†"
    else
        print_error "psycopg2ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—"
        ((upload_errors++))
    fi
    
    if [ $upload_errors -gt 0 ]; then
        print_error "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (${upload_errors}ä»¶)"
        exit 1
    fi
    
    print_success "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†"
}

# CloudFormationãƒ‡ãƒ—ãƒ­ã‚¤
deploy_cloudformation() {
    print_step "=== CloudFormationãƒ‡ãƒ—ãƒ­ã‚¤ ==="
    
    if check_stack_status; then
        # æ–°è¦ä½œæˆ
        print_info "æ–°è¦ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆé–‹å§‹..."
        print_warning "ã“ã®å‡¦ç†ã«ã¯10-15åˆ†ã‹ã‹ã‚Šã¾ã™ï¼ˆRDSä½œæˆã®ãŸã‚ï¼‰"
        
        # aws cloudformation deployã®ä»£ã‚ã‚Šã«create-stackã¨waitã‚’ä½¿ç”¨
        if aws cloudformation create-stack \
            --stack-name "${STACK_NAME}" \
            --template-body "file://${TEMPLATE_FILE}" \
            --parameters \
                ParameterKey=ProjectName,ParameterValue="${PROJECT_NAME}" \
                ParameterKey=DBMasterPassword,ParameterValue="${DB_PASSWORD}" \
            --capabilities CAPABILITY_IAM \
            --region "${REGION}" > /dev/null 2>&1; then
            print_success "ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡å®Œäº†"
            
            print_info "ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆå®Œäº†å¾…æ©Ÿä¸­..."
            print_warning "RDSä½œæˆã®ãŸã‚10-15åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."
            
            # é€²è¡ŒçŠ¶æ³è¡¨ç¤º
            local wait_count=0
            while [ $wait_count -lt 60 ]; do  # æœ€å¤§30åˆ†å¾…æ©Ÿ
                STACK_STATUS=$(aws cloudformation describe-stacks \
                    --stack-name "${STACK_NAME}" \
                    --region "${REGION}" \
                    --query 'Stacks[0].StackStatus' \
                    --output text 2>/dev/null || echo "UNKNOWN")
                
                case "$STACK_STATUS" in
                    "CREATE_COMPLETE")
                        echo ""
                        print_success "æ–°è¦ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆå®Œäº†"
                        return 0
                        ;;
                    "CREATE_FAILED"|"ROLLBACK_COMPLETE"|"ROLLBACK_FAILED")
                        echo ""
                        print_error "ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆå¤±æ•— (çŠ¶æ…‹: ${STACK_STATUS})"
                        print_info "å¤±æ•—ç†ç”±ç¢ºèªä¸­..."
                        
                        # å¤±æ•—ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã®è©³ç´°ã‚’è¡¨ç¤º
                        echo ""
                        echo "=== å¤±æ•—ã—ãŸãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ ==="
                        aws cloudformation describe-stack-events \
                            --stack-name "${STACK_NAME}" \
                            --region "${REGION}" \
                            --query 'StackEvents[?ResourceStatus==`CREATE_FAILED`].{Time:Timestamp,Resource:LogicalResourceId,Reason:ResourceStatusReason}' \
                            --output table 2>/dev/null || echo "è©³ç´°æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
                        
                        echo ""
                        echo "=== æœ€æ–°ã®ã‚¹ã‚¿ãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæœ€æ–°10ä»¶ï¼‰ ==="
                        aws cloudformation describe-stack-events \
                            --stack-name "${STACK_NAME}" \
                            --region "${REGION}" \
                            --max-items 10 \
                            --query 'StackEvents[].{Time:Timestamp,Status:ResourceStatus,Resource:LogicalResourceId,Reason:ResourceStatusReason}' \
                            --output table 2>/dev/null || echo "ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
                        
                        echo ""
                        print_info "CloudFormationã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§è©³ç´°ã‚’ç¢ºèª:"
                        echo "https://console.aws.amazon.com/cloudformation/home?region=${REGION}#/stacks?filteringStatus=active&filteringText=&viewNested=true&hideStacks=false"
                        
                        return 1
                        ;;
                    "CREATE_IN_PROGRESS")
                        echo -n "."
                        ;;
                    *)
                        echo -n "?"
                        ;;
                esac
                
                sleep 30
                ((wait_count++))
            done
            
            echo ""
            print_error "ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ30åˆ†çµŒéŽï¼‰"
            print_info "ç¾åœ¨ã®çŠ¶æ…‹: ${STACK_STATUS}"
            return 1
        else
            print_error "ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡å¤±æ•—"
            return 1
        fi
    else
        # æ›´æ–°
        print_info "æ—¢å­˜ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–°é–‹å§‹..."
        
        if deploy_with_changeset; then
            print_success "ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–°å®Œäº†"
            return 0
        elif [ $? -eq 2 ]; then
            print_info "å¤‰æ›´ãŒãªã„ãŸã‚æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ"
            return 0
        else
            print_error "ã‚¹ã‚¿ãƒƒã‚¯æ›´æ–°å¤±æ•—"
            return 1
        fi
    fi
}

# ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—
get_resource_info() {
    print_step "=== ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾— ==="
    
    # CloudFormationã‚¹ã‚¿ãƒƒã‚¯ã®å‡ºåŠ›å€¤ã‚’å–å¾—
    local outputs=$(aws cloudformation describe-stacks \
        --stack-name "${STACK_NAME}" \
        --region "${REGION}" \
        --query 'Stacks[0].Outputs' \
        --output json 2>/dev/null)
    
    if [ -z "$outputs" ] || [ "$outputs" = "null" ]; then
        print_error "ã‚¹ã‚¿ãƒƒã‚¯å‡ºåŠ›å€¤ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
        return 1
    fi
    
    # å„å‡ºåŠ›å€¤ã‚’å¤‰æ•°ã«æ ¼ç´
    S3_BUCKET=$(echo "$outputs" | python3 -c "import sys, json; data=json.load(sys.stdin); print(next((item['OutputValue'] for item in data if item['OutputKey']=='S3BucketName'), 'NOT_FOUND'))" 2>/dev/null || echo "NOT_FOUND")
    
    TABLE_CREATOR_FUNCTION_NAME=$(echo "$outputs" | python3 -c "import sys, json; data=json.load(sys.stdin); print(next((item['OutputValue'] for item in data if item['OutputKey']=='TableCreatorFunction'), 'NOT_FOUND'))" 2>/dev/null || echo "NOT_FOUND")
    
    CSV_PROCESSOR_FUNCTION_NAME=$(echo "$outputs" | python3 -c "import sys, json; data=json.load(sys.stdin); print(next((item['OutputValue'] for item in data if item['OutputKey']=='CSVProcessorFunction'), 'NOT_FOUND'))" 2>/dev/null || echo "NOT_FOUND")
    
    QUERY_EXECUTOR_FUNCTION_NAME=$(echo "$outputs" | python3 -c "import sys, json; data=json.load(sys.stdin); print(next((item['OutputValue'] for item in data if item['OutputKey']=='QueryExecutorFunction'), 'NOT_FOUND'))" 2>/dev/null || echo "NOT_FOUND")
    
    RDS_ENDPOINT=$(echo "$outputs" | python3 -c "import sys, json; data=json.load(sys.stdin); print(next((item['OutputValue'] for item in data if item['OutputKey']=='RDSEndpoint'), 'NOT_FOUND'))" 2>/dev/null || echo "NOT_FOUND")
    
    RDS_PORT=$(echo "$outputs" | python3 -c "import sys, json; data=json.load(sys.stdin); print(next((item['OutputValue'] for item in data if item['OutputKey']=='RDSPort'), 'NOT_FOUND'))" 2>/dev/null || echo "NOT_FOUND")
    
    # å–å¾—çµæžœç¢ºèª
    if [ "$S3_BUCKET" = "NOT_FOUND" ] || [ "$TABLE_CREATOR_FUNCTION_NAME" = "NOT_FOUND" ]; then
        print_error "å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
        print_debug "åˆ©ç”¨å¯èƒ½ãªå‡ºåŠ›å€¤:"
        echo "$outputs" | python3 -m json.tool 2>/dev/null || echo "$outputs"
        return 1
    fi
    
    print_success "ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—å®Œäº†"
    print_debug "S3ãƒã‚±ãƒƒãƒˆ: ${S3_BUCKET}"
    print_debug "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambda: ${TABLE_CREATOR_FUNCTION_NAME}"
    print_debug "CSVå‡¦ç†Lambda: ${CSV_PROCESSOR_FUNCTION_NAME}"
    print_debug "ã‚¯ã‚¨ãƒªå®Ÿè¡ŒLambda: ${QUERY_EXECUTOR_FUNCTION_NAME}"
    print_debug "RDSã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ${RDS_ENDPOINT}:${RDS_PORT}"
    
    return 0
}

# æœ¬ç•ªS3ãƒã‚±ãƒƒãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
move_files_to_production() {
    print_step "=== æœ¬ç•ªS3ãƒã‚±ãƒƒãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹• ==="
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
    print_info "psycopg2ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ä¸­..."
    if aws s3 cp "s3://${TEMP_BUCKET_NAME}/layers/psycopg2-layer-python311-fixed.zip" \
        "s3://${S3_BUCKET}/layers/psycopg2-layer-python311-fixed.zip" \
        --region ${REGION} > /dev/null 2>&1; then
        print_success "ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å®Œäº†"
    else
        print_warning "ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆLambdaé–¢æ•°ã¯æ—¢ã«ä½œæˆæ¸ˆã¿ã®ãŸã‚å½±éŸ¿ãªã—ï¼‰"
    fi
}

# SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
upload_sql_files() {
    if [ "$SQL_FILES_EXIST" != true ]; then
        print_warning "SQLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™"
        return 0
    fi
    
    print_step "=== SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ==="
    
    local sql_upload_count=0
    for sql_file in *.sql; do
        if [ -f "$sql_file" ]; then
            print_info "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: $sql_file"
            if aws s3 cp "$sql_file" "s3://${S3_BUCKET}/init-sql/$sql_file" --region ${REGION} > /dev/null 2>&1; then
                print_success "  âœ“ $sql_file"
                ((sql_upload_count++))
            else
                print_error "  âœ— $sql_file ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—"
            fi
        fi
    done
    
    if [ $sql_upload_count -gt 0 ]; then
        print_success "SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: ${sql_upload_count}ä»¶"
    else
        print_warning "SQLãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
    fi
}

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Ÿè¡Œ
execute_table_creation() {
    if [ "$SQL_FILES_EXIST" != true ]; then
        print_warning "SQLãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™"
        return 0
    fi
    
    print_step "=== ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Ÿè¡Œ ==="
    
    print_info "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambdaå®Ÿè¡Œä¸­..."
    print_warning "ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™"
    
    # Lambdaé–¢æ•°å®Ÿè¡Œ
    local table_result_file="table_creation_response.json"
    if aws lambda invoke \
        --function-name "${TABLE_CREATOR_FUNCTION_NAME}" \
        --payload '{}' \
        --region "${REGION}" \
        "$table_result_file" > /dev/null 2>&1; then
        
        print_success "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambdaå®Ÿè¡Œå®Œäº†"
        
        # çµæžœè¡¨ç¤º
        if [ -f "$table_result_file" ]; then
            print_info "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆçµæžœ:"
            if command -v python3 &> /dev/null; then
                cat "$table_result_file" | python3 -m json.tool 2>/dev/null || cat "$table_result_file"
            else
                cat "$table_result_file"
            fi
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if grep -q '"error"' "$table_result_file" 2>/dev/null; then
                print_warning "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            elif grep -q '"created_tables"' "$table_result_file" 2>/dev/null; then
                CREATED_COUNT=$(grep -o '"created_tables":\[.*\]' "$table_result_file" | grep -o ',' | wc -l)
                CREATED_COUNT=$((CREATED_COUNT + 1))
                print_success "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæˆåŠŸ: ${CREATED_COUNT}å€‹"
            fi
        else
            print_warning "çµæžœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        fi
    else
        print_error "ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambdaå®Ÿè¡Œå¤±æ•—"
        print_info "Lambdaé–¢æ•°ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„:"
        echo "aws logs tail /aws/lambda/${TABLE_CREATOR_FUNCTION_NAME} --follow --region ${REGION}"
    fi
}

# ãƒ†ã‚¹ãƒˆç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ
create_test_payloads() {
    print_step "=== ãƒ†ã‚¹ãƒˆç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ ==="
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆç”¨
    cat > test_table_creation.json << EOF
{}
EOF
    
    # CSVå‡¦ç†ãƒ†ã‚¹ãƒˆç”¨ï¼ˆæ¨¡æ“¬S3ã‚¤ãƒ™ãƒ³ãƒˆï¼‰
    cat > test_csv_processing.json << EOF
{
  "Records": [
    {
      "s3": {
        "bucket": {
          "name": "${S3_BUCKET}"
        },
        "object": {
          "key": "csv/test20250612.csv"
        }
      }
    }
  ]
}
EOF
    
    # ã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒ†ã‚¹ãƒˆç”¨
    cat > test_query_execution.json << EOF
{
  "sql": "SELECT version();",
  "output_format": "json",
  "output_name": "version_check"
}
EOF
    
    print_success "ãƒ†ã‚¹ãƒˆç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆå®Œäº†"
}

# æŽ¥ç¶šæ€§ãƒ†ã‚¹ãƒˆ
test_connectivity() {
    print_step "=== æŽ¥ç¶šæ€§ãƒ†ã‚¹ãƒˆ ==="
    
    # Lambda to RDSæŽ¥ç¶šãƒ†ã‚¹ãƒˆ
    print_info "Lambda -> RDSæŽ¥ç¶šãƒ†ã‚¹ãƒˆä¸­..."
    
    local connectivity_result_file="connectivity_test_result.json"
    if aws lambda invoke \
        --function-name "${QUERY_EXECUTOR_FUNCTION_NAME}" \
        --payload file://test_query_execution.json \
        --region "${REGION}" \
        "$connectivity_result_file" > /dev/null 2>&1; then
        
        print_success "Lambdaé–¢æ•°å®Ÿè¡Œå®Œäº†"
        
        # çµæžœç¢ºèª
        if [ -f "$connectivity_result_file" ]; then
            if grep -q "PostgreSQL" "$connectivity_result_file" 2>/dev/null || grep -q "version" "$connectivity_result_file" 2>/dev/null; then
                print_success "âœ… Lambda -> RDSæŽ¥ç¶š: OK"
                
                # PostgreSQLãƒãƒ¼ã‚¸ãƒ§ãƒ³è¡¨ç¤º
                if command -v python3 &> /dev/null; then
                    VERSION_INFO=$(cat "$connectivity_result_file" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    if 'body' in data:
        body = json.loads(data['body'])
        print('PostgreSQLæŽ¥ç¶šæˆåŠŸ')
    else:
        print('æŽ¥ç¶šç¢ºèªå®Œäº†')
except:
    print('çµæžœè§£æžã‚¨ãƒ©ãƒ¼')
" 2>/dev/null || echo "çµæžœè§£æžã‚¨ãƒ©ãƒ¼")
                    print_debug "$VERSION_INFO"
                fi
            else
                print_warning "âš ï¸  Lambda -> RDSæŽ¥ç¶š: è¦ç¢ºèª"
                print_debug "å¿œç­”å†…å®¹:"
                cat "$connectivity_result_file" | head -n 5
            fi
        fi
    else
        print_warning "Lambdaé–¢æ•°å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    fi
    
    # S3ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
    print_info "Lambda -> S3æŽ¥ç¶šãƒ†ã‚¹ãƒˆä¸­..."
    
    # ãƒ†ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    echo "test_column,data_column" > test_connectivity.csv
    echo "test_value,$(date)" >> test_connectivity.csv
    
    if aws s3 cp test_connectivity.csv "s3://${S3_BUCKET}/csv/test_connectivity.csv" --region ${REGION} > /dev/null 2>&1; then
        print_success "ãƒ†ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†"
        
        # å°‘ã—å¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒ­ã‚°ç¢ºèª
        print_info "CSVå‡¦ç†çµæžœå¾…æ©Ÿä¸­..."
        sleep 15
        
        # CSVå‡¦ç†ãƒ­ã‚°ç¢ºèª
        print_info "CSVå‡¦ç†ãƒ­ã‚°ç¢ºèªä¸­..."
        if aws logs filter-log-events \
            --log-group-name "/aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME}" \
            --start-time $(date -d '5 minutes ago' +%s)000 \
            --region "${REGION}" \
            --query 'events[].message' \
            --output text 2>/dev/null | grep -q "CSVå‡¦ç†å®Œäº†"; then
            print_success "âœ… S3 -> Lambda CSVå‡¦ç†: OK"
        else
            print_warning "âš ï¸  S3 -> Lambda CSVå‡¦ç†: ãƒ­ã‚°ç¢ºèªãŒå¿…è¦"
            print_debug "æœ€æ–°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„:"
            print_debug "aws logs tail /aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME} --since 5m --region ${REGION}"
        fi
    else
        print_warning "ãƒ†ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
    fi
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    rm -f test_connectivity.csv
    aws s3 rm "s3://${S3_BUCKET}/csv/test_connectivity.csv" --region ${REGION} > /dev/null 2>&1 || true
    
    print_success "æŽ¥ç¶šæ€§ãƒ†ã‚¹ãƒˆå®Œäº†"
}

# é‹ç”¨ã‚¬ã‚¤ãƒ‰ç”Ÿæˆ
generate_operations_guide() {
    print_step "=== é‹ç”¨ã‚¬ã‚¤ãƒ‰ç”Ÿæˆ ==="
    
    cat > operations_guide.md << EOF
# ETL CSV to RDS PostgreSQL System é‹ç”¨ã‚¬ã‚¤ãƒ‰

## ðŸ“‹ ãƒ‡ãƒ—ãƒ­ã‚¤æƒ…å ±
- **ãƒ‡ãƒ—ãƒ­ã‚¤æ—¥æ™‚**: $(date)
- **ã‚¹ã‚¿ãƒƒã‚¯å**: ${STACK_NAME}
- **ãƒªãƒ¼ã‚¸ãƒ§ãƒ³**: ${REGION}
- **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå**: ${PROJECT_NAME}
- **S3ãƒã‚±ãƒƒãƒˆ**: ${S3_BUCKET}
- **RDSã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**: ${RDS_ENDPOINT}:${RDS_PORT}
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å**: postgres
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¦ãƒ¼ã‚¶ãƒ¼**: postgres

## ðŸš€ Lambdaé–¢æ•°ä¸€è¦§

### 1. ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambda
- **é–¢æ•°å**: \`${TABLE_CREATOR_FUNCTION_NAME}\`
- **ç”¨é€”**: SQLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
- **ãƒˆãƒªã‚¬ãƒ¼**: æ‰‹å‹•å®Ÿè¡Œ
- **å®Ÿè¡Œæ–¹æ³•**:
\`\`\`bash
aws lambda invoke \\
  --function-name ${TABLE_CREATOR_FUNCTION_NAME} \\
  --payload '{}' \\
  --region ${REGION} \\
  result.json && cat result.json | python3 -m json.tool
\`\`\`

### 2. CSVå‡¦ç†Lambda
- **é–¢æ•°å**: \`${CSV_PROCESSOR_FUNCTION_NAME}\`
- **ç”¨é€”**: S3ã®CSVãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•å‡¦ç†ã—ã¦RDSã«æŠ•å…¥
- **ãƒˆãƒªã‚¬ãƒ¼**: S3ã®\`csv/\`ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®CSVãƒ•ã‚¡ã‚¤ãƒ«æŠ•å…¥
- **å®Ÿè¡Œæ–¹æ³•**:
\`\`\`bash
# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨è‡ªå‹•å®Ÿè¡Œ
aws s3 cp sample.csv s3://${S3_BUCKET}/csv/sample.csv --region ${REGION}

# å‡¦ç†çŠ¶æ³ç¢ºèª
aws logs tail /aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME} --follow --region ${REGION}
\`\`\`

### 3. ã‚¯ã‚¨ãƒªå®Ÿè¡ŒLambda
- **é–¢æ•°å**: \`${QUERY_EXECUTOR_FUNCTION_NAME}\`
- **ç”¨é€”**: ä»»æ„SQLã®å®Ÿè¡Œã¨çµæžœS3å‡ºåŠ›
- **ãƒˆãƒªã‚¬ãƒ¼**: æ‰‹å‹•å®Ÿè¡Œ
- **å®Ÿè¡Œæ–¹æ³•**:
\`\`\`bash
# åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªå®Ÿè¡Œ
aws lambda invoke \\
  --function-name ${QUERY_EXECUTOR_FUNCTION_NAME} \\
  --payload '{"sql":"SELECT COUNT(*) FROM information_schema.tables;","output_format":"json","output_name":"table_count"}' \\
  --region ${REGION} \\
  result.json && cat result.json | python3 -m json.tool

# CSVå½¢å¼ã§ã®å‡ºåŠ›
aws lambda invoke \\
  --function-name ${QUERY_EXECUTOR_FUNCTION_NAME} \\
  --payload '{"sql":"SELECT table_name FROM information_schema.tables WHERE table_schema='"'"'public'"'"';","output_format":"csv","output_name":"table_list"}' \\
  --region ${REGION} \\
  result.json
\`\`\`

## ðŸ“ S3ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ
\`\`\`
s3://${S3_BUCKET}/
â”œâ”€â”€ csv/               # CSVæŠ•å…¥ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆè‡ªå‹•å‡¦ç†ã•ã‚Œã‚‹ï¼‰
â”œâ”€â”€ init-sql/          # åˆæœŸãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆSQL
â”œâ”€â”€ query-results/     # ã‚¯ã‚¨ãƒªçµæžœå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€
â””â”€â”€ layers/            # Lambda Layerï¼ˆpsycopg2ï¼‰
\`\`\`

## ðŸ”§ åŸºæœ¬æ“ä½œæ‰‹é †

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
1. SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ \`init-sql/\` ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
\`\`\`bash
aws s3 cp create_tables.sql s3://${S3_BUCKET}/init-sql/ --region ${REGION}
\`\`\`

2. ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambdaå®Ÿè¡Œ
\`\`\`bash
aws lambda invoke --function-name ${TABLE_CREATOR_FUNCTION_NAME} --payload '{}' --region ${REGION} result.json
\`\`\`

### CSVãƒ‡ãƒ¼ã‚¿æŠ•å…¥
1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ \`csv/\` ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè‡ªå‹•ã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
\`\`\`bash
aws s3 cp sales_data.csv s3://${S3_BUCKET}/csv/ --region ${REGION}
\`\`\`

2. å‡¦ç†çŠ¶æ³ç¢ºèª
\`\`\`bash
aws logs tail /aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME} --since 5m --region ${REGION}
\`\`\`

### ãƒ‡ãƒ¼ã‚¿åˆ†æžãƒ»ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
1. é›†è¨ˆã‚¯ã‚¨ãƒªå®Ÿè¡Œä¾‹
\`\`\`bash
aws lambda invoke \\
  --function-name ${QUERY_EXECUTOR_FUNCTION_NAME} \\
  --payload '{"sql":"SELECT department, COUNT(*) as count, AVG(salary) as avg_salary FROM employees GROUP BY department ORDER BY count DESC;","output_format":"csv","output_name":"department_stats"}' \\
  --region ${REGION} \\
  result.json
\`\`\`

2. çµæžœç¢ºèª
\`\`\`bash
aws s3 ls s3://${S3_BUCKET}/query-results/ --region ${REGION}
aws s3 cp s3://${S3_BUCKET}/query-results/department_stats_YYYYMMDD_HHMMSS.csv . --region ${REGION}
\`\`\`

## ðŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Lambdaé–¢æ•°ã®ãƒ­ã‚°ç¢ºèª
\`\`\`bash
# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆLambda
aws logs tail /aws/lambda/${TABLE_CREATOR_FUNCTION_NAME} --follow --region ${REGION}

# CSVå‡¦ç†Lambda
aws logs tail /aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME} --follow --region ${REGION}

# ã‚¯ã‚¨ãƒªå®Ÿè¡ŒLambda
aws logs tail /aws/lambda/${QUERY_EXECUTOR_FUNCTION_NAME} --follow --region ${REGION}
\`\`\`

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›´æŽ¥æŽ¥ç¶š
\`\`\`bash
# psqlã§ã®ç›´æŽ¥æŽ¥ç¶šï¼ˆVPCå†…ã‹ã‚‰ã®ã¿å¯èƒ½ï¼‰
psql -h ${RDS_ENDPOINT} -p ${RDS_PORT} -U postgres -d postgres

# ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ç¢ºèª
psql -h ${RDS_ENDPOINT} -p ${RDS_PORT} -U postgres -d postgres -c "\\dt"

# æŽ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆLambdaçµŒç”±ï¼‰
aws lambda invoke \\
  --function-name ${QUERY_EXECUTOR_FUNCTION_NAME} \\
  --payload '{"sql":"SELECT version();","output_format":"json"}' \\
  --region ${REGION} \\
  version_check.json
\`\`\`

### S3ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
\`\`\`bash
# ãƒã‚±ãƒƒãƒˆå†…å®¹ç¢ºèª
aws s3 ls s3://${S3_BUCKET}/ --recursive --region ${REGION}

# ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã®ç¢ºèª
aws s3 ls s3://${S3_BUCKET}/csv/ --region ${REGION}
aws s3 ls s3://${S3_BUCKET}/query-results/ --region ${REGION}
\`\`\`

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»VPCç¢ºèª
\`\`\`bash
# VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¢ºèª
aws ec2 describe-vpc-endpoints --region ${REGION} --query 'VpcEndpoints[?VpcId==\`$(aws cloudformation describe-stacks --stack-name ${STACK_NAME} --region ${REGION} --query 'Stacks[0].Outputs[?OutputKey==\`VPCId\`].OutputValue' --output text)\`].{Service:ServiceName,State:State}'

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ç¢ºèª
aws ec2 describe-security-groups --region ${REGION} --filters Name=group-name,Values='*etl-csv-to-rds-postgresql*'
\`\`\`

## ðŸ“Š ç›£è¦–ãƒ»é‹ç”¨

### CloudWatch Logsç›£è¦–
- Lambdaé–¢æ•°ã®å®Ÿè¡Œãƒ­ã‚°ã¯30æ—¥é–“ä¿æŒ
- ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯CloudWatch Logsã§è©³ç´°ç¢ºèª

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ç›£è¦–
- RDS Performance InsightsãŒæœ‰åŠ¹ï¼ˆ7æ—¥é–“ä¿æŒï¼‰
- Lambdaé–¢æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- RDSã®è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆ7æ—¥é–“ä¿æŒï¼‰
- S3ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°æœ‰åŠ¹

## ðŸ” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢
- ã™ã¹ã¦ã®ãƒªã‚½ãƒ¼ã‚¹ãŒãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚µãƒ–ãƒãƒƒãƒˆã«é…ç½®
- VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã§AWSã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã¸ã®ç›´æŽ¥é€šä¿¡ãªã—

### ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
- IAMãƒ­ãƒ¼ãƒ«ã«ã‚ˆã‚‹æœ€å°æ¨©é™ã®åŽŸå‰‡
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã§ãƒãƒ¼ãƒˆåˆ¶é™
- S3ãƒã‚±ãƒƒãƒˆã®ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹ç„¡åŠ¹

## ðŸš€ å°†æ¥ã®æ‹¡å¼µ

### ã‚¯ãƒ­ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚¢ã‚¯ã‚»ã‚¹å¯¾å¿œ
S3ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã‚’æ›´æ–°ã—ã¦å¤–éƒ¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯:
\`\`\`json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowCrossAccountPut",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::OTHER-ACCOUNT-ID:root"
      },
      "Action": [
        "s3:PutObject",
        "s3:PutObjectAcl"
      ],
      "Resource": "arn:aws:s3:::${S3_BUCKET}/csv/*",
      "Condition": {
        "StringEquals": {
          "s3:x-amz-acl": "bucket-owner-full-control"
        }
      }
    }
  ]
}
\`\`\`

### ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ
- LambdaåŒæ™‚å®Ÿè¡Œæ•°ã®èª¿æ•´
- RDSã®åž‚ç›´/æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- S3ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†
- Aurora Serverlessã¸ã®ç§»è¡Œæ¤œè¨Ž

## ðŸ—‘ï¸ ã‚·ã‚¹ãƒ†ãƒ å‰Šé™¤

ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤æ™‚ã®æ‰‹é †:
\`\`\`bash
# ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤
aws cloudformation delete-stack --stack-name ${STACK_NAME} --region ${REGION}

# ä¸€æ™‚ãƒã‚±ãƒƒãƒˆå‰Šé™¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
aws s3 rm s3://etl-csv-to-rds-postgresql-temp-files-${ACCOUNT_ID} --recursive --region ${REGION}
aws s3 rb s3://etl-csv-to-rds-postgresql-temp-files-${ACCOUNT_ID} --region ${REGION}
\`\`\`

---
**ç”Ÿæˆæ—¥æ™‚**: $(date)  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒ**: ${REGION}
EOF

    print_success "é‹ç”¨ã‚¬ã‚¤ãƒ‰ç”Ÿæˆå®Œäº†: operations_guide.md"
}

# è¨­å®šæƒ…å ±ä¿å­˜
save_deployment_info() {
    print_step "=== è¨­å®šæƒ…å ±ä¿å­˜ ==="
    
    cat > deployment_info.txt << EOF
# ETL CSV to RDS PostgreSQL System ãƒ‡ãƒ—ãƒ­ã‚¤æƒ…å ±
# ç”Ÿæˆæ—¥æ™‚: $(date)

# === åŸºæœ¬è¨­å®š ===
STACK_NAME="${STACK_NAME}"
REGION="${REGION}"
PROJECT_NAME="${PROJECT_NAME}"
ACCOUNT_ID="${ACCOUNT_ID}"

# === ãƒªã‚½ãƒ¼ã‚¹æƒ…å ± ===
S3_BUCKET="${S3_BUCKET}"
TABLE_CREATOR_FUNCTION_NAME="${TABLE_CREATOR_FUNCTION_NAME}"
CSV_PROCESSOR_FUNCTION_NAME="${CSV_PROCESSOR_FUNCTION_NAME}"
QUERY_EXECUTOR_FUNCTION_NAME="${QUERY_EXECUTOR_FUNCTION_NAME}"
RDS_ENDPOINT="${RDS_ENDPOINT}"
RDS_PORT="${RDS_PORT}"

# === ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š ===
DB_USER="postgres"
DB_PASSWORD="${DB_PASSWORD}"
DB_NAME="postgres"

# === ä¸€æ™‚ãƒã‚±ãƒƒãƒˆ ===
TEMP_BUCKET_NAME="etl-csv-to-rds-postgresql-temp-files-${ACCOUNT_ID}"

# === ã‚¯ã‚¤ãƒƒã‚¯ã‚³ãƒžãƒ³ãƒ‰ ===

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
aws lambda invoke --function-name ${TABLE_CREATOR_FUNCTION_NAME} --payload '{}' --region ${REGION} result.json

# CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè‡ªå‹•å‡¦ç†ï¼‰
aws s3 cp sample.csv s3://${S3_BUCKET}/csv/sample.csv --region ${REGION}

# ãƒ‡ãƒ¼ã‚¿ç¢ºèª
aws lambda invoke --function-name ${QUERY_EXECUTOR_FUNCTION_NAME} --payload '{"sql":"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='"'"'public'"'"';","output_format":"json"}' --region ${REGION} count.json

# ãƒ­ã‚°ç¢ºèª
aws logs tail /aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME} --follow --region ${REGION}

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›´æŽ¥æŽ¥ç¶š
psql -h ${RDS_ENDPOINT} -p ${RDS_PORT} -U postgres -d postgres

# ã‚¹ã‚¿ãƒƒã‚¯å‰Šé™¤
aws cloudformation delete-stack --stack-name ${STACK_NAME} --region ${REGION}
EOF

    print_success "è¨­å®šæƒ…å ±ä¿å­˜å®Œäº†: deployment_info.txt"
}

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
cleanup_temp_files() {
    print_step "=== ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ ==="
    
    local temp_files=(
        "table_creator.zip"
        "csv_processor.zip"
        "query_executor.zip"
        "test_table_creation.json"
        "test_csv_processing.json"
        "test_query_execution.json"
        "table_creation_response.json"
        "connectivity_test_result.json"
    )
    
    local cleaned_count=0
    for temp_file in "${temp_files[@]}"; do
        if [ -f "$temp_file" ]; then
            rm -f "$temp_file"
            ((cleaned_count++))
        fi
    done
    
    if [ $cleaned_count -gt 0 ]; then
        print_success "ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†: ${cleaned_count}ä»¶"
    else
        print_debug "å‰Šé™¤å¯¾è±¡ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    fi
}

# æœ€çµ‚çµæžœè¡¨ç¤º
show_deployment_summary() {
    print_step "=== ãƒ‡ãƒ—ãƒ­ã‚¤çµæžœã‚µãƒžãƒªãƒ¼ ==="
    
    echo ""
    echo "ðŸŽ‰ =============================================="
    echo "ðŸŽ‰   ETL CSV to RDS PostgreSQL System"
    echo "ðŸŽ‰        ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ï¼"
    echo "ðŸŽ‰ =============================================="
    echo ""
    
    echo "ðŸ“‹ **ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±**"
    echo "   âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¢ãƒ¼ãƒ‰: ${DEPLOY_MODE:-UPDATE}"
    echo "   âœ… ã‚¹ã‚¿ãƒƒã‚¯å: ${STACK_NAME}"
    echo "   âœ… ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: ${REGION}"
    echo "   âœ… S3ãƒã‚±ãƒƒãƒˆ: ${S3_BUCKET}"
    echo "   âœ… RDSã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ${RDS_ENDPOINT}:${RDS_PORT}"
    echo ""
    
    echo "ðŸ”§ **Lambdaé–¢æ•°ï¼ˆ3ã¤ï¼‰**"
    echo "   1. ðŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ: ${TABLE_CREATOR_FUNCTION_NAME}"
    echo "   2. ðŸ“Š CSVå‡¦ç†: ${CSV_PROCESSOR_FUNCTION_NAME}"
    echo "   3. ðŸ” ã‚¯ã‚¨ãƒªå®Ÿè¡Œ: ${QUERY_EXECUTOR_FUNCTION_NAME}"
    echo ""
    
    echo "ðŸ“ **S3ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ**"
    echo "   ðŸ“ csv/           â†’ CSVãƒ•ã‚¡ã‚¤ãƒ«æŠ•å…¥ï¼ˆè‡ªå‹•å‡¦ç†ï¼‰"
    echo "   ðŸ“ init-sql/      â†’ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆSQL"
    echo "   ðŸ“ query-results/ â†’ ã‚¯ã‚¨ãƒªçµæžœå‡ºåŠ›"
    echo "   ðŸ“ layers/        â†’ Lambda Layer"
    echo ""
    
    if [ "$SQL_FILES_EXIST" = true ]; then
        echo "âœ… **SQLãƒ•ã‚¡ã‚¤ãƒ«**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Ÿè¡Œæ¸ˆã¿"
    else
        echo "âš ï¸  **SQLãƒ•ã‚¡ã‚¤ãƒ«**: è¦‹ã¤ã‹ã‚‰ãšï¼ˆå¾Œã§æ‰‹å‹•è¿½åŠ å¯èƒ½ï¼‰"
    fi
    echo ""
    
    echo "ðŸš€ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**"
    echo "   1. é‹ç”¨ã‚¬ã‚¤ãƒ‰ç¢ºèª: cat operations_guide.md"
    echo "   2. æŽ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:"
    echo "      aws lambda invoke --function-name ${QUERY_EXECUTOR_FUNCTION_NAME} \\"
    echo "        --payload '{\"sql\":\"SELECT version();\",\"output_format\":\"json\"}' \\"
    echo "        --region ${REGION} version.json"
    echo "   3. CSVãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥:"
    echo "      aws s3 cp test.csv s3://${S3_BUCKET}/csv/test.csv --region ${REGION}"
    echo ""
    
    echo "ðŸ“– **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**"
    echo "   ðŸ“„ é‹ç”¨ã‚¬ã‚¤ãƒ‰: operations_guide.md"
    echo "   ðŸ“„ è¨­å®šæƒ…å ±: deployment_info.txt"
    echo ""
    
    echo "ðŸ”§ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**"
    echo "   ðŸ“ ãƒ­ã‚°ç¢ºèª: aws logs tail /aws/lambda/${CSV_PROCESSOR_FUNCTION_NAME} --follow --region ${REGION}"
    echo "   ðŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶š: psql -h ${RDS_ENDPOINT} -p ${RDS_PORT} -U postgres -d postgres"
    echo "   ðŸ” S3å†…å®¹ç¢ºèª: aws s3 ls s3://${S3_BUCKET}/ --recursive --region ${REGION}"
    echo ""
    
    echo "ðŸ’¡ **é‡è¦ãªç‰¹å¾´**"
    echo "   ðŸ”’ å®Œå…¨ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆç’°å¢ƒï¼ˆVPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨ï¼‰"
    echo "   ðŸš€ NATã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤ä¸è¦ï¼ˆã‚³ã‚¹ãƒˆåŠ¹çŽ‡çš„ï¼‰"
    echo "   ðŸ“Š 3ã¤ã®Lambdaé–¢æ•°ã§å½¹å‰²åˆ†é›¢"
    echo "   ðŸ”„ S3ãƒˆãƒªã‚¬ãƒ¼ã«ã‚ˆã‚‹è‡ªå‹•CSVå‡¦ç†"
    echo "   ðŸ“ˆ å°†æ¥ã®ã‚¯ãƒ­ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå¯¾å¿œæº–å‚™æ¸ˆã¿"
    echo ""
    
    echo "ðŸŽ¯ **ã‚·ã‚¹ãƒ†ãƒ å‰Šé™¤æ–¹æ³•**"
    echo "   aws cloudformation delete-stack --stack-name ${STACK_NAME} --region ${REGION}"
    echo ""
    
    echo "ðŸŽ‰ ãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ ðŸŽ‰"
    echo ""
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
main() {
    print_info "=== ETL CSV to RDS PostgreSQL System ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®Œå…¨ç‰ˆãƒ»æ”¹å–„ç‰ˆï¼‰ ==="
    print_info "é–‹å§‹æ—¥æ™‚: $(date)"
    echo ""
    
    # Step 1: å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
    check_prerequisites
    echo ""
    
    # Step 2: VPCè¨­å®šæ¤œè¨¼
    validate_vpc_configuration
    echo ""
    
    # Step 3: å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    SQL_FILES_EXIST=true
    check_required_files || SQL_FILES_EXIST=false
    echo ""
    
    # Step 4: Lambdaé–¢æ•°ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    create_lambda_zips
    echo ""
    
    # Step 5: æ—¢å­˜ã‚¹ã‚¿ãƒƒã‚¯ç¢ºèªãƒ»ä¸€æ™‚ãƒã‚±ãƒƒãƒˆç®¡ç†
    manage_temp_bucket
    echo ""
    
    # Step 6: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_files
    echo ""
    
    # Step 7: CloudFormationãƒ‡ãƒ—ãƒ­ã‚¤
    if deploy_cloudformation; then
        DEPLOY_MODE="SUCCESS"
        echo ""
        
        # Step 8: ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±å–å¾—
        if get_resource_info; then
            echo ""
            
            # Step 9: æœ¬ç•ªS3ãƒã‚±ãƒƒãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
            move_files_to_production
            echo ""
            
            # Step 10: SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            upload_sql_files
            echo ""
            
            # Step 11: ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Ÿè¡Œ
            execute_table_creation
            echo ""
            
            # Step 12: ãƒ†ã‚¹ãƒˆç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆãƒ»æŽ¥ç¶šæ€§ãƒ†ã‚¹ãƒˆ
            create_test_payloads
            test_connectivity
            echo ""
            
            # Step 13: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
            generate_operations_guide
            save_deployment_info
            echo ""
            
            # Step 14: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            cleanup_temp_files
            echo ""
            
            # Step 15: æœ€çµ‚çµæžœè¡¨ç¤º
            show_deployment_summary
            
        else
            print_error "ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
            exit 1
        fi
    else
        print_error "CloudFormationãƒ‡ãƒ—ãƒ­ã‚¤ã«å¤±æ•—ã—ã¾ã—ãŸ"
        exit 1
    fi
}

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
main "$@"

exit 0